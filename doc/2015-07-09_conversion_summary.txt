CUL FULL CATALOG MARCXML-TO-BIBFRAME CONVERSION 
July 9, 2015 (Delivery 1)


--------------------------------------------------------------------------------
VM SPECS
--------------------------------------------------------------------------------

Amazon EC2 c3.8xlarge
Public DNS: ec2-52-7-88-24.compute-1.amazonaws.com

32 CPU
60GB RAM

Java heap size: -Xms24g -Xmx56g



--------------------------------------------------------------------------------
MARCXML INPUT
--------------------------------------------------------------------------------

Location: http://da-data.library.cornell.edu/bibdata/
Generation date: 2015-05-30

The files contain suppressed records as well as unsuppressed. In order to filter
out the suppressed records, suppression status would have to be obtained by
querying the Voyager database or through the daily list generated by the daily
catalog update processes. The suppressed files were NOT removed for this
conversion.

Total number of records: 7,199,390
Total size of all files: 33GB
Original number of files: 179

Since currently we have no error recovery included in the conversion process, 
any file containing an error does not get processed, and an empty RDF file is 
generated. As a short term solution, the records were broken up into files of 
200 records each, so that only 200 records would be dropped for every error. 

Resulting number of files: 36,078



--------------------------------------------------------------------------------
RDF OUTPUT
--------------------------------------------------------------------------------

Date of conversion: 2015-07-09/10
RDF serialization: ntriples
XSLT and XQuery processor: Saxon 9.5

Total number of RDF files generated: 36,078
Number of non-empty files: 34,540
Number of empty files: 1538
Number of dropped records: 30,760
Percent dropped records: 4.27%

Total number of triples:  948,971,342

The 36,078 files were broken up into 6 directories to be processed by 6 
parallel processes. Details on the files and processing time:

Directory 1: 5999 files in 17:19:51 hours
Directory 2: 5999 files in 16:54:22 hours
Directory 3: 5999 files in 17:25:54 hours
Directory 4: 5999 files in 18:09:56 hours
Directory 5: 5999 files in 23:23:48 hours
Directory 6: 6083 files in 20:50:33 hours



--------------------------------------------------------------------------------
POTENTIAL IMPROVEMENTS
--------------------------------------------------------------------------------

Processing time
~~~~~~~~~~~~~~~

A Ruby wrapper was used to control the processing, and calls Java from the 
commandline for each file, so Java is continuously stopping and starting. 
Presumably processing time could be significantly reduced by using an all-Java 
process. 

Point for investigation: each process maxed out at 500% of CPU, even when the
other processes were using less. What is preventing more CPU from being used?



Architecture
~~~~~~~~~~~~

Aside from reduced processing time, it would be desirable to have a single,
end-to-end process that would also incorporate pre- and post-processing. Such 
a one-button operation would make it easier for other users and repeated 
processing. 

Some benchmarking should be done to determine whether Ruby (using either the
RDF.rb or rdf-raptor gem) or Java processes RDF files and models (the
latter for post-processing) more efficiently. See 
https://github.com/ruby-rdf/rdf/issues/118 for one comparison of Java and
RDF.rb. rdf-raptor uses the C-based Raptor parsers and is expected to be
significantly faster.

For LD4L purposes it is probably not worth the time investment, however. 



Error recovery
~~~~~~~~~~~~~~

Use Kevin Ford's Java wrapper for error recovery, so that only the record with
the error (or the part of the record with the error?) is dropped.

Alternatively, use Zorba, which has built-in error recovery.



--------------------------------------------------------------------------------
COMPARISON WITH HARVARD AND STANFORD
--------------------------------------------------------------------------------

Harvard: 
24 CPU server
13 million records, broken up into files of 200 records each
14 parallel processes
29 hours

Stanford:
7.5 million records
75 files, approximately 1GB each
Java heap size: -Xms24g -Xmx52g
40-60 minutes per process = 1.5 days
Includes pre- and post-processing

